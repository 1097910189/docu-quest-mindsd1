# NotebookLM RAG Backend

Sistema backend para NotebookLM con integraciÃ³n de Milvus, HuggingFace Embeddings y LLM remoto.

## CaracterÃ­sticas

- ğŸ§  **RAG (Retrieval-Augmented Generation)** con Milvus como base vectorial
- ğŸ“„ **Procesamiento de documentos** (PDF, TXT, DOCX, MD)
- ğŸ” **BÃºsqueda semÃ¡ntica** con embeddings de HuggingFace
- ğŸ¤– **IntegraciÃ³n con LLM remoto** (compatible con OpenAI API)
- ğŸ”— **APIs REST** para integraciÃ³n con n8n y frontend
- ğŸ³ **Dockerizado** para fÃ¡cil despliegue

## InstalaciÃ³n y ConfiguraciÃ³n

### 1. ConfiguraciÃ³n Local

```bash
# Instalar dependencias
pip install -r requirements.txt

# Configurar variables de entorno (opcional)
export MILVUS_HOST=localhost
export MILVUS_PORT=19530
export LLM_ENDPOINT=http://localhost:8001/v1
```

### 2. Usando Docker Compose (Recomendado)

```bash
# Levantar todos los servicios
docker-compose up -d

# Ver logs
docker-compose logs -f notebook-lm-api

# Detener servicios
docker-compose down
```

### 3. Solo Milvus con Docker

```bash
# Levantar solo Milvus
docker-compose up -d etcd minio milvus

# Ejecutar API localmente
python main.py
```

## ConfiguraciÃ³n del LLM

### Gemma 3-12B con Ollama

```bash
# Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Descargar Gemma
ollama pull gemma:12b

# Ejecutar con API compatible OpenAI
ollama serve --port 8001
```

### Gemma 3-12B con vLLM

```bash
# Instalar vLLM
pip install vllm

# Ejecutar servidor
python -m vllm.entrypoints.openai.api_server \
  --model google/gemma-2-12b-it \
  --port 8001 \
  --served-model-name gemma
```

### ConfiguraciÃ³n Manual LLM

Si tienes tu propio servidor, asegÃºrate de que sea compatible con la API de OpenAI y actualiza la variable `LLM_ENDPOINT` en el cÃ³digo.

## API Endpoints

### POST /api/upload
Sube y procesa documentos.

```bash
curl -X POST "http://localhost:8000/api/upload" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@documento.pdf" \
  -F "embedding_model=sentence-transformers/all-MiniLM-L6-v2"
```

### POST /api/ask
Hace preguntas sobre los documentos.

```bash
curl -X POST "http://localhost:8000/api/ask" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Â¿CuÃ¡les son los puntos principales del documento?",
    "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
    "top_k": 5
  }'
```

### GET /api/documents
Lista documentos indexados.

```bash
curl "http://localhost:8000/api/documents"
```

### DELETE /api/documents/{document_id}
Elimina un documento.

```bash
curl -X DELETE "http://localhost:8000/api/documents/{document_id}"
```

### POST /api/test-milvus
Prueba conexiÃ³n a Milvus.

```bash
curl -X POST "http://localhost:8000/api/test-milvus" \
  -H "Content-Type: application/json" \
  -d '{"milvus_host": "localhost:19530"}'
```

## IntegraciÃ³n con n8n

### ConfiguraciÃ³n BÃ¡sica

1. **Instalar n8n:**
```bash
npm install -g n8n
n8n start
```

2. **Crear workflow de ejemplo:**
   - Importa los workflows desde `n8n_examples.json`
   - Configura las URLs de los endpoints
   - Ajusta los parÃ¡metros segÃºn tus necesidades

### Workflows Incluidos

1. **Upload Document**: Subida automÃ¡tica de documentos
2. **Ask Question**: Servicio de preguntas y respuestas
3. **Batch Processing**: Procesamiento en lote de documentos
4. **Question Answering Service**: Servicio completo de Q&A

### Ejemplo de Nodo HTTP en n8n

**Para subir documentos:**
- URL: `http://localhost:8000/api/upload`
- MÃ©todo: POST
- Body Type: Form-Data
- ParÃ¡metros:
  - `file`: [archivo]
  - `embedding_model`: `sentence-transformers/all-MiniLM-L6-v2`

**Para hacer preguntas:**
- URL: `http://localhost:8000/api/ask`
- MÃ©todo: POST
- Body Type: JSON
- Body:
```json
{
  "question": "{{ $json.question }}",
  "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
  "top_k": 5
}
```

## Modelos de Embeddings Recomendados

- **all-MiniLM-L6-v2**: RÃ¡pido y eficiente (384 dimensiones)
- **all-mpnet-base-v2**: Mayor calidad (768 dimensiones)
- **paraphrase-multilingual-MiniLM-L12-v2**: Soporte multilingÃ¼e
- **e5-small-v2**: Optimizado para bÃºsqueda

## Troubleshooting

### Error de conexiÃ³n a Milvus
```bash
# Verificar que Milvus estÃ© corriendo
docker ps | grep milvus

# Revisar logs
docker-compose logs milvus
```

### Error de memoria con embeddings
```bash
# Usar modelo mÃ¡s pequeÃ±o
# Cambiar a: sentence-transformers/all-MiniLM-L6-v2
```

### Error de conexiÃ³n al LLM
```bash
# Verificar endpoint
curl http://localhost:8001/v1/models

# Revisar logs del LLM
```

## Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend  â”‚â”€â”€â”€â–¶â”‚  FastAPI    â”‚â”€â”€â”€â–¶â”‚   Milvus    â”‚
â”‚   React     â”‚    â”‚   Backend   â”‚    â”‚  Vector DB  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ HuggingFace â”‚    â”‚  Gemma 3    â”‚
                   â”‚ Embeddings  â”‚    â”‚  LLM Remote â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ContribuciÃ³n

1. Fork el repositorio
2. Crea una rama para tu feature
3. Commit tus cambios
4. Push a la rama
5. Abre un Pull Request

## Licencia

MIT License